{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d533098d-06a9-40b7-9400-b9d1203e2e1a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;36m  File \u001B[0;32m\"<command-3420688240022827>\"\u001B[0;36m, line \u001B[0;32m2\u001B[0m\n",
       "\u001B[0;31m    from azure-storage.blob import BlobServiceClient\u001B[0m\n",
       "\u001B[0m              ^\u001B[0m\n",
       "\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;36m  File \u001B[0;32m\"<command-3420688240022827>\"\u001B[0;36m, line \u001B[0;32m2\u001B[0m\n\u001B[0;31m    from azure-storage.blob import BlobServiceClient\u001B[0m\n\u001B[0m              ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n",
       "errorSummary": "<span class='ansi-red-fg'>SyntaxError</span>: invalid syntax (<command-3420688240022827>, line 2)",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Source: https://www.cloudfronts.com/azure/azure-databricks-how-to-read-csv-file-from-blob-storage-#and-push-the-data-into-a-synapse-sql-pool-table/\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "import pandas as pd\n",
    "\n",
    "import io\n",
    "\n",
    "import pyspark.sql\n",
    "\n",
    "storage_account_name = \"capstone606storage\"\n",
    "\n",
    "storage_account_access_key = 'U9/wqUO7SoqF1pTv4W9hp7yP1kAc36f4825UX5KsVLbeMfEf/JNN3iglTUGnX/MYjf7FyWTweqoR+ASthrjt4g=='\n",
    "\n",
    "spark.conf.set('fs.azure.account.key.' + storage_account_name + '.blob.core.windows.net', storage_account_access_key)\n",
    "\n",
    "blob_container = 'storagecontainer'\n",
    "\n",
    "filePath = \"wasbs://\" + blob_container + \"@\" + storage_account_name + \".blob.core.windows.net/u1008.csv\"\n",
    "\n",
    "empDf = spark.read.format(\"csv\").load(filePath, inferSchema = True, header = True)\n",
    "\n",
    "empDf.show(10)\n",
    "# connectionString=”DefaultEndpointsProtocol=https;AccountName=capstone606storage;AccountKey=U9/wqUO7SoqF1pTv4W9hp7yP1kAc36f4825UX5KsVLbeMfEf/JNN3iglTUGnX/MYjf7FyWTweqoR+ASthrjt4g==;EndpointSuffix=core.windows.net”\n",
    "\n",
    "# empDf.write.jdbc(connectionString,”[dbo].[Employee]”, mode=”append”)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6da75541-62f5-4443-b171-22c6ca6696ec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)\n",
       "\u001B[0;32m<command-3420688240022828>\u001B[0m in \u001B[0;36m<cell line: 13>\u001B[0;34m()\u001B[0m\n",
       "\u001B[1;32m     11\u001B[0m \u001B[0mblob_name\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m\"u1008\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m     12\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0;32m---> 13\u001B[0;31m \u001B[0mdf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"com.microsoft.azure.storage.blob\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0m\u001B[1;32m     14\u001B[0m     \u001B[0;34m.\u001B[0m\u001B[0moption\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"spark.storage.blockManagerTimeoutIntervalMs\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"200000\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m     15\u001B[0m     \u001B[0;34m.\u001B[0m\u001B[0moption\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"fs.azure.account.key.{0}.blob.core.windows.net\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0maccount_name\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maccount_key\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\n",
       "\u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m             \u001B[0mstart\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m     47\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0;32m---> 48\u001B[0;31m                 \u001B[0mres\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0m\u001B[1;32m     49\u001B[0m                 logger.log_success(\n",
       "\u001B[1;32m     50\u001B[0m                     \u001B[0mmodule_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mclass_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfunction_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mstart\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msignature\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\n",
       "\u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py\u001B[0m in \u001B[0;36mload\u001B[0;34m(self, path, format, schema, **options)\u001B[0m\n",
       "\u001B[1;32m    175\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moptions\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m**\u001B[0m\u001B[0moptions\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m    176\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0;32m--> 177\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_df\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jreader\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0m\u001B[1;32m    178\u001B[0m         \u001B[0;32melif\u001B[0m \u001B[0mpath\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m    179\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mtype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m!=\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\n",
       "\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1319\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m   1320\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0;32m-> 1321\u001B[0;31m         return_value = get_return_value(\n",
       "\u001B[0m\u001B[1;32m   1322\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n",
       "\u001B[1;32m   1323\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
       "\n",
       "\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    194\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mdeco\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mAny\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mAny\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mAny\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m    195\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0;32m--> 196\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0m\u001B[1;32m    197\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mPy4JJavaError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m    198\u001B[0m             \u001B[0mconverted\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mconvert_exception\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjava_exception\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\n",
       "\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py\u001B[0m in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n",
       "\u001B[1;32m    324\u001B[0m             \u001B[0mvalue\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mOUTPUT_CONVERTER\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mtype\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgateway_client\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m    325\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mREFERENCE_TYPE\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0;32m--> 326\u001B[0;31m                 raise Py4JJavaError(\n",
       "\u001B[0m\u001B[1;32m    327\u001B[0m                     \u001B[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m    328\u001B[0m                     format(target_id, \".\", name), value)\n",
       "\n",
       "\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o549.load.\n",
       ": java.lang.ClassNotFoundException: \n",
       "Failed to find data source: com.microsoft.azure.storage.blob. Please find packages at\n",
       "https://spark.apache.org/third-party-projects.html\n",
       "       \n",
       "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedToFindDataSourceError(QueryExecutionErrors.scala:837)\n",
       "\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:744)\n",
       "\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:794)\n",
       "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:328)\n",
       "\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:237)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
       "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n",
       "\tat py4j.Gateway.invoke(Gateway.java:306)\n",
       "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
       "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
       "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n",
       "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "Caused by: java.lang.ClassNotFoundException: com.microsoft.azure.storage.blob.DefaultSource\n",
       "\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\n",
       "\tat java.lang.ClassLoader.loadClass(ClassLoader.java:419)\n",
       "\tat java.lang.ClassLoader.loadClass(ClassLoader.java:352)\n",
       "\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:730)\n",
       "\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:730)\n",
       "\tat scala.util.Failure.orElse(Try.scala:224)\n",
       "\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:730)\n",
       "\t... 15 more\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)\n\u001B[0;32m<command-3420688240022828>\u001B[0m in \u001B[0;36m<cell line: 13>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     11\u001B[0m \u001B[0mblob_name\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m\"u1008\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 13\u001B[0;31m \u001B[0mdf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"com.microsoft.azure.storage.blob\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     14\u001B[0m     \u001B[0;34m.\u001B[0m\u001B[0moption\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"spark.storage.blockManagerTimeoutIntervalMs\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"200000\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     15\u001B[0m     \u001B[0;34m.\u001B[0m\u001B[0moption\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"fs.azure.account.key.{0}.blob.core.windows.net\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0maccount_name\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0maccount_key\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m             \u001B[0mstart\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     47\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 48\u001B[0;31m                 \u001B[0mres\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     49\u001B[0m                 logger.log_success(\n\u001B[1;32m     50\u001B[0m                     \u001B[0mmodule_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mclass_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfunction_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mstart\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msignature\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py\u001B[0m in \u001B[0;36mload\u001B[0;34m(self, path, format, schema, **options)\u001B[0m\n\u001B[1;32m    175\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moptions\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m**\u001B[0m\u001B[0moptions\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    176\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 177\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_df\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jreader\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    178\u001B[0m         \u001B[0;32melif\u001B[0m \u001B[0mpath\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    179\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mtype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m!=\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1319\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1320\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1321\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1322\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1323\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    194\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mdeco\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mAny\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mAny\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mAny\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    195\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 196\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    197\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mPy4JJavaError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    198\u001B[0m             \u001B[0mconverted\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mconvert_exception\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjava_exception\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py\u001B[0m in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    324\u001B[0m             \u001B[0mvalue\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mOUTPUT_CONVERTER\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mtype\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgateway_client\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    325\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0manswer\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mREFERENCE_TYPE\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 326\u001B[0;31m                 raise Py4JJavaError(\n\u001B[0m\u001B[1;32m    327\u001B[0m                     \u001B[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    328\u001B[0m                     format(target_id, \".\", name), value)\n\n\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o549.load.\n: java.lang.ClassNotFoundException: \nFailed to find data source: com.microsoft.azure.storage.blob. Please find packages at\nhttps://spark.apache.org/third-party-projects.html\n       \n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedToFindDataSourceError(QueryExecutionErrors.scala:837)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:744)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:794)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:328)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:237)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.ClassNotFoundException: com.microsoft.azure.storage.blob.DefaultSource\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:387)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:419)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:352)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:730)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:730)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:730)\n\t... 15 more\n",
       "errorSummary": "java.lang.ClassNotFoundException: ",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Capstone606\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.microsoft.azure:azure-storage:8.6.6\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "account_name = \"capstone606storage\"\n",
    "account_key = \"U9/wqUO7SoqF1pTv4W9hp7yP1kAc36f4825UX5KsVLbeMfEf/JNN3iglTUGnX/MYjf7FyWTweqoR+ASthrjt4g==\"\n",
    "container_name = \"storagecontainer\"\n",
    "blob_name = \"u1008\"\n",
    "\n",
    "df = spark.read.format(\"com.microsoft.azure.storage.blob\") \\\n",
    "    .option(\"spark.storage.blockManagerTimeoutIntervalMs\", \"200000\") \\\n",
    "    .option(\"fs.azure.account.key.{0}.blob.core.windows.net\".format(account_name), account_key) \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .load(\"wasbs://{0}@{1}.blob.core.windows.net/{2}/{3}\"\n",
    "          .format(container_name, account_name, blob_name, blob_name+\".csv\"))\n",
    "\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68297aa0-f976-49c7-8cdf-27d4b713f196",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[9]: True"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.dbutils import DBUtils\n",
    "\n",
    "# import necessary libraries\n",
    "from pyspark.sql.types import *\n",
    "# pyspark functions\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Capstone606\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Replace with your own account and blob information\n",
    "account_name = dbutils.secrets.get(scope='capstonescope', key='accountName')\n",
    "account_key = dbutils.secrets.get(scope='capstonescope', key='key')\n",
    "container_name = dbutils.secrets.get(scope='capstonescope', key='containerName')\n",
    "\n",
    "folder_name=\"clickstreamfiles\"\n",
    "#blob_name = \"u1008\"\n",
    "\n",
    "dbutils.fs.mount(\n",
    "source = \"wasbs://{0}@{1}.blob.core.windows.net\".format(container_name, account_name),\n",
    "    mount_point= \"/mnt/capstone606\",\n",
    "    extra_configs = {\"fs.azure.account.key.{0}.blob.core.windows.net\".format(account_name):\n",
    "                    dbutils.secrets.get(scope='capstonescope', key='key')}\n",
    "    \n",
    ")  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f28695c-ea2d-4d28-a20d-fc97ac07f23a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------------+----------------------+-------------------+---------------+-----------+----------------+\n|username|num_of_questions_answered|num_of_correct_answers|num_of_explanations|num_of_lectures|devicesUsed|confidence_level|\n+--------+-------------------------+----------------------+-------------------+---------------+-----------+----------------+\n| u100043|                      605|                   264|                412|             22|       null|              24|\n|  u10019|                     1761|                  1412|               1766|              3|     mobile|             105|\n|  u10020|                      169|                   115|                119|              2|       null|              19|\n|  u10021|                      136|                    96|                114|              1|       null|               3|\n|  u10026|                       60|                    42|                 65|              3|       null|              48|\n|  u10030|                      179|                   127|                148|              5|       null|              20|\n|  u10032|                      306|                   183|                303|             48|       null|               0|\n|  u10033|                        0|                     0|                  1|              0|       null|               0|\n|  u10037|                        6|                     3|                  0|              0|     mobile|               0|\n|  u10046|                        6|                     4|                  0|              0|        web|               1|\n+--------+-------------------------+----------------------+-------------------+---------------+-----------+----------------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, countDistinct, max, collect_list, sum\n",
    "\n",
    "# Create an empty PySpark DataFrame with the required columns\n",
    "schema = \"username STRING, num_of_questions_answered INT, num_of_correct_answers INT, num_of_explanations INT, num_of_lectures INT, devicesUsed STRING, confidence_level INT\"\n",
    "mainDf1 = spark.createDataFrame([], schema)\n",
    "questionsdf = spark.read.csv(\"/mnt/capstone606/questionsandpayments/questions.csv\",header=True,inferSchema=True)\n",
    "paymentsdf = spark.read.csv(\"/mnt/capstone606/questionsandpayments/payments.csv\",header=True,inferSchema=True)\n",
    "\n",
    "# File Schema\n",
    "file_schema = StructType([\n",
    "    StructField('timestamp', StringType(), False),\n",
    "    StructField('action_type', StringType(), True),\n",
    "    StructField('item_id', StringType(), True),\n",
    "    StructField('cursor_time', LongType(), True),\n",
    "    StructField('source', StringType(), True),\n",
    "    StructField('user_answer', StringType(), True),\n",
    "    StructField('platform', StringType(), True),\n",
    "    StructField('user_id', StringType(), True)\n",
    "])\n",
    "\n",
    "#schema_1 = \"timestamp STRING. action_type STRING, item_id STRING, cursor_time INT, source STRING, user_answer STRING, platform STRING\"\n",
    "for i in dbutils.fs.ls(\"/mnt/capstone606/clickstreamfiles/\"):\n",
    "    df = spark.read.csv(path= i.path, header=True, schema=file_schema)\n",
    "    num_of_questions_answered = df.filter(col('action_type') == 'respond').agg(countDistinct('item_id')).collect()[0][0]\n",
    "    \n",
    "    #rows with the lastest reponse\n",
    "    latest_responses = df.filter(col('action_type') == 'respond').select('item_id', 'timestamp',            'user_answer').groupBy('item_id').agg(max('timestamp').alias('timestamp'), max('user_answer').alias('user_answer'))\n",
    "    latest_responses = latest_responses.sort('item_id')\n",
    "    # Extracting the questions correct answers\n",
    "#     tempdf = questionsdf.filter(col('question_id').isin(latest_responses.select('item_id').my_rdd.flatMap(lambda x: x)).alias('alias1')).select('question_id', 'correct_answer').sort('question_id')\n",
    "    #questions_list = list(latest_responses.select('item_id'))\n",
    "    #tempdf = questionsdf.filter(col('question_id').isin(questions_list).alias('alias1')).select('question_id', 'correct_answer').sort('question_id')\n",
    "    questions_id = latest_responses.select('item_id').rdd.flatMap(lambda x: x).collect()\n",
    "    tempdf = questionsdf.filter(col('question_id').isin(questions_id)).select('question_id', 'correct_answer').sort('question_id')\n",
    "\n",
    "    tempdf = tempdf.withColumnRenamed('correct_answer', 'correct_answer_temp')\n",
    "    latest_responses = latest_responses.join(tempdf, latest_responses.item_id == tempdf.question_id).select('item_id', 'timestamp', 'user_answer', 'correct_answer_temp')\n",
    "    #Comparing the list of responses with correct answers\n",
    "    num_of_correct_answers = latest_responses.filter(col('user_answer') == col('correct_answer_temp')).count()\n",
    "    # Explanations and Lecture\n",
    "    num_of_explanations = df.filter(col('item_id').rlike('e')).agg(countDistinct('item_id')).collect()[0][0]\n",
    "    num_of_lectures = df.filter(col('item_id').rlike('l')).agg(countDistinct('item_id')).collect()[0][0]\n",
    "    #additional metric\n",
    "    # First: Type of devices\n",
    "    device = df.select('platform').distinct().collect()[0][0]\n",
    "    #Second: Confidence level by \"erase_choice\" and adaptive offers\n",
    "    confidence_level = df.filter(col('action_type') == 'erase_choice').count()\n",
    "    #Third: tags explored by the users\n",
    "    # How many paygo and pass\n",
    "#     payments = paymentsdf.filter(col('payment_item_id').isin(df.filter(col('action_type') == 'pay').select('item_id').rdd.flatMap(lambda x: x).collect())).select('type').rdd.flatMap(lambda x: x).collect()\n",
    "    # Duration\n",
    "    #duration = paymentsdf.filter(col('payment_item_id').isin(df.filter(col('action_type') == 'pay').select('item_id'))).agg(sum('duaration')).collect()[0][0]\n",
    "    #Username\n",
    "    username = str(i.path.split('/')[-1]).split('.')[0]\n",
    "    #Putting values in global dataframe\n",
    "    mainDf1 = mainDf1.union(spark.createDataFrame([(username, num_of_questions_answered, num_of_correct_answers, num_of_explanations, num_of_lectures, device, confidence_level)], schema))\n",
    "    \n",
    "    df.unpersist() # to avoid any memory issues\n",
    "\n",
    "mainDf1.cache()\n",
    "mainDf1.show(10)  \n",
    "    \n",
    "\n",
    "# UNMOUNT\n",
    "#dbutils.fs.unmount(\"/mnt/capstone606\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47001d59-87f2-4c95-9e9a-600c8c1b7026",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------------+----------------------+-------------------+---------------+-----------+----------------+\n|username|num_of_questions_answered|num_of_correct_answers|num_of_explanations|num_of_lectures|devicesUsed|confidence_level|\n+--------+-------------------------+----------------------+-------------------+---------------+-----------+----------------+\n| u100043|                      605|                   264|                412|             22|       null|              24|\n|  u10019|                     1761|                  1412|               1766|              3|     mobile|             105|\n|  u10020|                      169|                   115|                119|              2|       null|              19|\n|  u10021|                      136|                    96|                114|              1|       null|               3|\n|  u10026|                       60|                    42|                 65|              3|       null|              48|\n|  u10030|                      179|                   127|                148|              5|       null|              20|\n|  u10032|                      306|                   183|                303|             48|       null|               0|\n|  u10033|                        0|                     0|                  1|              0|       null|               0|\n|  u10037|                        6|                     3|                  0|              0|     mobile|               0|\n|  u10046|                        6|                     4|                  0|              0|        web|               1|\n+--------+-------------------------+----------------------+-------------------+---------------+-----------+----------------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "# OPTIMIZED VERSION 1\n",
    "from pyspark.sql.functions import countDistinct, max, sum\n",
    "\n",
    "# Create an empty PySpark DataFrame with the required columns\n",
    "schema = \"username STRING, num_of_questions_answered INT, num_of_correct_answers INT, num_of_explanations INT, num_of_lectures INT, devicesUsed STRING, confidence_level INT\"\n",
    "mainDf1 = spark.createDataFrame([], schema)\n",
    "\n",
    "questionsdf = spark.read.csv(\"/mnt/capstone606/questionsandpayments/questions.csv\", header=True, inferSchema=True)\n",
    "paymentsdf = spark.read.csv(\"/mnt/capstone606/questionsandpayments/payments.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# File Schema\n",
    "file_schema = \"timestamp STRING, action_type STRING, item_id STRING, cursor_time LONG, source STRING, user_answer STRING, platform STRING, user_id STRING\"\n",
    "j=0\n",
    "for i in dbutils.fs.ls(\"/mnt/capstone606/clickstreamfiles/\"):\n",
    "    df = spark.read.csv(i.path, header=True, schema=file_schema)\n",
    "\n",
    "    num_of_questions_answered = df.filter(\"action_type = 'respond'\").agg(countDistinct('item_id')).collect()[0][0]\n",
    "\n",
    "    # rows with the lastest reponse\n",
    "    latest_responses = df.filter(\"action_type = 'respond'\") \\\n",
    "                         .groupBy('item_id') \\\n",
    "                         .agg(max('timestamp').alias('timestamp'), max('user_answer').alias('user_answer')) \\\n",
    "                         .select('item_id', 'timestamp', 'user_answer')\n",
    "\n",
    "    # join with questionsdf to get correct answers\n",
    "    latest_responses = latest_responses.join(questionsdf, latest_responses.item_id == questionsdf.question_id) \\\n",
    "                                       .select('item_id', 'timestamp', 'user_answer', 'correct_answer') \\\n",
    "                                       .cache()\n",
    "\n",
    "    # Comparing the list of responses with correct answers\n",
    "    num_of_correct_answers = latest_responses.filter(\"user_answer = correct_answer\").count()\n",
    "\n",
    "    # Explanations and Lecture\n",
    "    num_of_explanations = df.filter(\"item_id rlike 'e'\").agg(countDistinct('item_id')).collect()[0][0]\n",
    "    num_of_lectures = df.filter(\"item_id rlike 'l'\").agg(countDistinct('item_id')).collect()[0][0]\n",
    "\n",
    "    # additional metrics\n",
    "    # First: Type of devices\n",
    "    device = df.select('platform').distinct().collect()[0][0]\n",
    "\n",
    "    # Second: Confidence level by \"erase_choice\" and adaptive offers\n",
    "    confidence_level = df.filter(\"action_type = 'erase_choice'\").count()\n",
    "    username = str(i.path.split('/')[-1]).split('.')[0]\n",
    "    #Putting values in global dataframe\n",
    "    mainDf1 = mainDf1.union(spark.createDataFrame([(username, num_of_questions_answered, num_of_correct_answers, num_of_explanations, num_of_lectures, device, confidence_level)], schema))\n",
    "    \n",
    "    df.unpersist() # to avoid any memory issues\n",
    "    j+=1\n",
    "    if(j==1000):\n",
    "        break\n",
    "\n",
    "mainDf1.cache()\n",
    "mainDf1.show(10)  \n",
    "\n",
    "\n",
    "mainDf1.write.format(\"delta\").mode(\"append\").option(\"path\",\"/mnt/capstone606/output/mainDf\").saveAsTable(\"default.mainDf1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "035357ca-92b1-4681-8f1a-e2b6632eafb6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mainDf1.write.parquet(\"/mnt/capstone606/output/mainDf.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c1e8805-a890-4a0a-b875-c27181024a20",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Capstone606\") \\\n",
    "    .getOrCreate()\n",
    "mainDf1 = spark.read.format('delta').load(\"/mnt/capstone606/output/mainDf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc57085d-d015-4f04-a6d1-a05942028d0f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 7\n"
     ]
    }
   ],
   "source": [
    "print(mainDf1.count(),len(mainDf1.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d646e5a-60e9-492e-804c-1cd5ab9aa3b2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+--------------+--------------+----+-------------+-------------+-------------------+\n|question_id|bundle_id|explanation_id|correct_answer|part|         tags|  deployed_at|           accuracy|\n+-----------+---------+--------------+--------------+----+-------------+-------------+-------------------+\n|      q1550|    b1450|         e1450|             a|   3|   54;183;182|1514566472641| 0.6666666666666666|\n|      q1927|    b1576|         e1576|             b|   3|   57;182;181|1514565378640|                1.0|\n|      q4527|    b3059|         e3059|             b|   5|          122|1565337118434|0.38461538461538464|\n|      q6915|    b5205|         e5205|             a|   6|           72|1558093775995|                0.5|\n|      q4538|    b3070|         e3070|             b|   5|           72|1565337136024|              0.875|\n|      q6164|    b4696|         e4696|             a|   5|           94|1564125530908|              0.125|\n|        q40|      b40|           e40|             b|   1|     11;7;181|1514558570368| 0.5416666666666666|\n|       q810|     b810|          e810|             a|   2|44;45;181;184|           -1| 0.6296296296296297|\n|      q5916|    b4448|         e4448|             b|   5|           84|1561367069954| 0.7142857142857143|\n|      q3894|    b2426|         e2426|             c|   5|           74|1534408381649|                0.5|\n+-----------+---------+--------------+--------------+----+-------------+-------------+-------------------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, lit, sum\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.dbutils import DBUtils\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Capstone606\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create an empty Spark DataFrame with the same schema as main_df\n",
    "schema = 'question_id STRING, correct_answer STRING, user_answer STRING, correct_flag INT, question_flag INT'\n",
    "file_schema = \"timestamp STRING, action_type STRING, item_id STRING, cursor_time LONG, source STRING, user_answer STRING, platform STRING, user_id STRING\"\n",
    "questions_df = spark.createDataFrame([], schema)\n",
    "\n",
    "# Read questions.csv as a Spark DataFrame\n",
    "questionsdf = spark.read.csv(\"/mnt/capstone606/questionsandpayments/questions.csv\",header=True,inferSchema=True)\n",
    "j=0\n",
    "# Loop through files in the EdNet-KT4/KT4 directory\n",
    "for i in dbutils.fs.ls(\"/mnt/capstone606/clickstreamfiles/\"):\n",
    "    # Read the file as a Spark DataFrame\n",
    "    df = spark.read.csv(path=i.path, header=True, schema=file_schema)\n",
    "\n",
    "    # Filter by action_type == 'respond'\n",
    "    respond_responses = df.filter(\"action_type = 'respond'\")\n",
    "\n",
    "    # Join with questionsdf on question_id to get correct_answer\n",
    "    rr = respond_responses.select('item_id').rdd.flatMap(lambda x: x).collect()\n",
    "    tempdf = questionsdf.select('question_id', 'correct_answer').\\\n",
    "             filter(col('question_id').isin(rr))\n",
    "    respond_responses2 = respond_responses.join(tempdf, respond_responses.item_id == tempdf.question_id)\n",
    "\n",
    "    # Drop unnecessary columns and rename user_answer to match main_df\n",
    "    respond_responses2 = respond_responses2.drop('item_id', 'timestamp', 'cursor_time', 'source', 'action_type', 'platform').\\\n",
    "                         withColumnRenamed('user_response', 'user_answer')\n",
    "\n",
    "    # Compute correct_flag and question_flag\n",
    "    respond_responses2 = respond_responses2.withColumn('correct_flag', \n",
    "                             (col('user_answer') == col('correct_answer')).cast(IntegerType())).\\\n",
    "                         withColumn('question_flag', lit(1))\n",
    "\n",
    "    # Append to main_df\n",
    "    questions_df = questions_df.union(respond_responses2.select(*questions_df.columns))\n",
    "    j+=1\n",
    "    if(j==100):\n",
    "        break\n",
    "questions_df.cache()\n",
    "\n",
    "\n",
    "questions_groupby = questions_df.groupBy('question_id').agg(\n",
    "    sum('correct_flag').alias('correct_flag_no'), \n",
    "    sum('question_flag').alias('question_flag_no')\n",
    ")\n",
    "#questions_groupby.show(10)\n",
    "\n",
    "questions_groupby2 = questions_df.join(questions_groupby, on='question_id', how='inner') \\\n",
    "                                .select([col(c) for c in questions_df.columns] + \n",
    "                                        [col('correct_flag_no'), col('question_flag_no')])\n",
    "questions_groupby2 = questionsdf.join(questions_groupby.select(\n",
    "    col('question_id'),\n",
    "    (col('correct_flag_no')/col('question_flag_no')).alias('accuracy')\n",
    "), on='question_id')\n",
    "\n",
    "questions_groupby2.cache()\n",
    "questions_groupby2.show(10)\n",
    "\n",
    "\n",
    "# questions_groupby2.write.format(\"delta\").mode(\"append\").option(\"path\",\"/mnt/capstone606/output/tables\").saveAsTable(\"default.questionsTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85e73535-a4b2-4b39-9542-58161a184f6b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "questions_groupby2.write.format(\"delta\").mode(\"append\").option(\"path\",\"/mnt/capstone606/output/tables\").saveAsTable(\"default.questionsTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99ad527f-1579-4b9c-88f0-97fd4e6b536b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "questions_groupby2.write.parquet(\"/mnt/capstone606/output/questions_groupby2Final.parquet\")\n",
    "\n",
    "# questions_groupby2.write.format(\"delta\").mode(\"append\").option(\"path\",\"/mnt/capstone606/output/tables\").saveAsTable(\"default.questionsTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5e7de4d-50a3-4a75-a43b-22be3780a84e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+-------------+\n|question_id|correct_flag|question_flag|\n+-----------+------------+-------------+\n|      q1550|           2|            3|\n|      q1927|           3|            3|\n|      q4527|          10|           26|\n|      q6915|           1|            2|\n|       q539|          19|           29|\n|        q15|           8|           15|\n|      q5064|           8|           10|\n|      q6056|           3|            7|\n|      q1146|          19|           24|\n|      q3831|           7|           11|\n+-----------+------------+-------------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "\n",
    "questions_groupby = questions_df.groupBy('question_id').agg(\n",
    "    sum('correct_flag').alias('correct_flag'), \n",
    "    sum('question_flag').alias('question_flag')\n",
    ")\n",
    "questions_groupby.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e53bbc64-9fdf-4363-8103-add632fd9686",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "\u001B[0;32m<command-4170612332572212>\u001B[0m in \u001B[0;36m<cell line: 18>\u001B[0;34m()\u001B[0m\n",
       "\u001B[1;32m     40\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m     41\u001B[0m     \u001B[0;31m# Append to questions_df\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0;32m---> 42\u001B[0;31m     \u001B[0mquestions_df\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mquestions_df\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0munion\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrespond_responses2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0m\u001B[1;32m     43\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m     44\u001B[0m     \u001B[0;31m# Limit the number of files processed to 100\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\n",
       "\u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m             \u001B[0mstart\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m     47\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0;32m---> 48\u001B[0;31m                 \u001B[0mres\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0m\u001B[1;32m     49\u001B[0m                 logger.log_success(\n",
       "\u001B[1;32m     50\u001B[0m                     \u001B[0mmodule_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mclass_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfunction_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mstart\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msignature\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\n",
       "\u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py\u001B[0m in \u001B[0;36munion\u001B[0;34m(self, other)\u001B[0m\n",
       "\u001B[1;32m   2546\u001B[0m         \u001B[0mAlso\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mstandard\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mSQL\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mthis\u001B[0m \u001B[0mfunction\u001B[0m \u001B[0mresolves\u001B[0m \u001B[0mcolumns\u001B[0m \u001B[0mby\u001B[0m \u001B[0mposition\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0;32mnot\u001B[0m \u001B[0mby\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m   2547\u001B[0m         \"\"\"\n",
       "\u001B[0;32m-> 2548\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mDataFrame\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0munion\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mother\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jdf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msparkSession\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0m\u001B[1;32m   2549\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m   2550\u001B[0m     \u001B[0;34m@\u001B[0m\u001B[0msince\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m1.3\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\n",
       "\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1319\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m   1320\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0;32m-> 1321\u001B[0;31m         return_value = get_return_value(\n",
       "\u001B[0m\u001B[1;32m   1322\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n",
       "\u001B[1;32m   1323\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
       "\n",
       "\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    200\u001B[0m                 \u001B[0;31m# Hide where the exception came from that shows a non-Pythonic\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m    201\u001B[0m                 \u001B[0;31m# JVM exception message.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0;32m--> 202\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mconverted\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[0m\u001B[1;32m    203\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\u001B[1;32m    204\u001B[0m                 \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: Union can only be performed on tables with the same number of columns, but the first table has 5 columns and the second table has 6 columns;\n",
       "'Union false, false\n",
       ":- LogicalRDD [question_id#618, correct_answer#619, user_answer#620, correct_flag#621, question_flag#622], false\n",
       "+- Project [user_answer#664, user_id#666, question_id#645, correct_answer#648, cast((user_answer#664 = correct_answer#648) as int) AS correct_flag#710, 1 AS question_flag#716]\n",
       "   +- Join Inner, (item_id#661 = question_id#645)\n",
       "      :- Filter (action_type#660 = respond)\n",
       "      :  +- Relation [timestamp#659,action_type#660,item_id#661,cursor_time#662L,source#663,user_answer#664,platform#665,user_id#666] csv\n",
       "      +- Filter question_id#645 IN (q4078,q6068,q6056,q4719,q6725,q6243,q5677,q4485,q4485,q5206,q6458,q4042,q342,q1190,q797,q997,q739,q1115,q1240,q1240,q925,q927,q200,q736,q1266,q6993,q6994,q6995,q6995,q6996,q6937,q6939,q6940,q6938,q6938,q6773,q6773,q6774,q6775,q6776,q6841,q6842,q6843,q6844,q604,q508,q776,q594,q856,q878,q1368,q1000,q447,q584,q584,q584,q1030,q462,q65,q65,q194,q154,q149,q8162,q115,q973,q1076,q406,q846,q1338,q312,q1045,q588,q1193,q956,q952,q651,q8140,q8057,q111,q8067,q160,q86,q1175,q1297,q854,q655,q655,q422,q696,q1190,q797,q997,q997,q739,q1240,q927,q736,q1266,q508,q763,q955,q216,q832,q835,q835,q835,q1101,q1219,q788,q1292,q576,q887,q539,q1213,q784,q1331,q774,q774,q583,q6650,q6651,q6652,q6649,q6650,q6836,q6833,q6833,q6834,q6834,q6834,q6835,q6913,q6913,q6914,q6915,q6916,q6809,q6812,q6812,q6812,q6810,q6811,q776,q594,q878,q584,q1030,q1030,q65,q149,q8162,q115,q406,q406,q1338,q312,q1045,q588,q1193,q1143,q1003,q1003,q949,q569,q1203,q1146,q1146,q1146,q732,q1224,q906,q1314,q1314,q1110,q1110,q1110,q355,q1554,q1556,q1555,q1555,q1555,q1556,q1556,q1758,q1759,q1760,q1759,q1759,q2439,q2440,q2441,q1632,q1633,q1634,q1634,q1722,q1723,q1724,q726,q439,q1218,q259,q803,q507,q1316,q590,q1376,q666,q566,q877,q1983,q1983,q1985,q1984,q2069,q2068,q2067,q1776,q1778,q1777,q1777,q1778,q2442,q2443,q2444,q2304,q2306,q2305,q956,q956,q952,q651,q8140,q8057,q8057,q160,q854,q655,q1250,q1114,q1336,q1339,q699,q724,q456,q1267,q1159,q1159,q752,q1187,q554,q2556,q2557,q2558,q2556,q1989,q1990,q1991,q2392,q2391,q2393,q1536,q1537,q1537,q1538,q1851,q1852,q1853,q1153,q905,q1251,q280,q1065,q217,q1280,q689,q1322,q292,q938,q1011,q1801,q1802,q1800,q1800,q1802,q2476,q2476,q2477,q2475,q2475,q1875,q1876,q1875,q1875,q1876,q1877,q1875,q2484,q2484,q2484,q2486,q2485,q2486,q2486,q2409,q2411,q2411,q2410,q2574,q2576,q2575,q2246,q2245,q2244,q1830,q1830,q1831,q1831,q1832,q1831,q1762,q1761,q1761,q1762,q1763,q1935,q1936,q1935,q1937,q1935,q1936,q722,q581,q285,q283,q1343,q1311,q1107,q397,q397,q1374,q1374,q251,q1234,q1234,q262,q696,q696,q1190,q1266,q740,q921,q633,q968,q375,q263,q263,q263,q763,q763,q216,q832,q835,q1292,q784,q2043,q2043,q2044,q2044,q2045,q1575,q1575,q1576,q1577,q2520,q2521,q2522,q2520,q2520,q1734,q1736,q1735,q1923,q1923,q1925,q1925,q1924,q561,q353,q353,q750,q573,q366,q423,q448,q291,q279,q1310,q612,q764,q764,q2505,q2505,q2505,q2506,q2505,q2507,q2379,q2380,q2381,q2380,q2381,q1791,q1792,q1793,q1793,q1785,q1786,q1786,q1786,q1787,q2340,q2341,q2342,q2340,q2340,q1677,q1678,q1678,q1679,q1766,q1765,q1764,q1764,q1764,q1890,q1891,q1891,q1892,q1616,q1616,q1614,q1615,q1548,q1549,q1550,q1550,q767,q773,q985,q289,q336,q634,q373,q799,q261,q261,q502,q1155,q915,q1917,q1919,q1918,q1918,q1918,q1919,q1918,q2166,q2166,q2167,q2168,q2168,q2166,q2167,q2168,q1608,q1610,q1609,q1609,q1610,q1610,q1587,q1587,q1589,q1589,q1589,q1588,q2259,q2260,q2261,q2261,q948,q392,q982,q442,q1345,q380,q895,q589,q848,q1111,q585,q639,q2397,q2397,q2397,q2398,q2398,q2398,q2398,q2399,q2399,q2362,q2363,q2361,q2363,q2363,q2363,q2363,q2362,q1953,q1954,q1955,q1955,q1955,q1955,q1545,q1547,q1546,q1546,q1546,q2226,q2227,q2227,q2227,q2228,q2226,q7861,q7865,q7864,q7863,q7862,q7475,q7476,q7477,q7478,q7479,q7478,q7054,q7056,q7057,q7057,q7058,q7058,q7055,q56,q8172,q20,q11075,q13,q13,q13,q120,q8152,q11106,q16,q8179,q8096,q8096,q8096,q11095,q193,q8134,q8123,q11143,q108,q165,q165,q8101,q11238,q608,q608,q719,q309,q11262,q11262,q11262,q494,q1015,q790,q11259,q350,q586,q586,q2343,q2344,q2345,q1926,q1927,q1928,q2547,q2548,q2549,q1740,q1741,q1742,q2295,q2296,q2297,q2823,q2823,q2825,q2824,q2825,q3115,q3116,q3114,q3414,q3415,q3416,q3416,q3416,q3450,q3452,q3451,q3450,q3450,q3451,q3451,q3452,q5054,q5054,q5780,q6360,q3114,q3115,q3116,q3116,q2823,q2824,q2825,q2928,q7402,q7402,q7402,q7402,q2208,q2208,q2208,q8095,q9007,q5530,q5530,q5530,q4827,q4526,q9114,q9114,q9114,q9114,q9114,q4468,q4128,q4128,q4128,q4631,q4631,q17386,q17386,q4008,q938,q2226,q3629,q6331,q6523,q6523,q6523,q6523,q6523,q6523,q3774,q3774,q3774,q5031,q3928,q4497,q5044,q5784,q5784,q5784,q5784,q6180,q6180,q6180,q5063,q4672,q5662,q3831,q5094,q5094,q5094,q5094,q5094,q5220,q5220,q4865,q17063,q17063,q5769,q5769,q5314,q4313,q4313,q4313,q8789,q6469,q6469,q5934,q5934,q6360,q5475,q1971,q1972,q1973,q1972,q2001,q2001,q2003,q2002,q1618,q1617,q1617,q1618,q1618,q1619,q2454,q2454,q2455,q2455,q2456,q2456,q1986,q1987,q1988,q1987,q1986,q6031,q6031,q6031,q6038,q6038,q6038,q4931,q4931,q4931,q4792,q4886,q4886,q6544,q6544,q3911,q3911,q5337,q5337,q5337,q5337,q6185,q5040,q5040,q6225,q10020,q5046,q5740,q5740,q5682,q5682,q5682,q5682,q4181,q4314,q4314,q6453,q6453,q6453,q5630,q5630,q4602,q5267,q9578,q9578,q9578,q9578,q9578,q9578,q5731,q5632,q4022,q6746,q5060,q6114,q6183,q6183,q5065,q1596,q1597,q1598,q1598,q2508,q2509,q2510,q1962,q1963,q1963,q1963,q1964,q2188,q2189,q2189,q2187,q2091,q2092,q2093,q2093,q5052,q4045,q4045,q3937,q3937,q5737,q5737,q5787,q5686,q5057,q5057,q4527,q4478,q4325,q4325,q6171,q6171,q6171,q6171,q5315,q5315,q5315,q3698,q5764,q5764,q5764,q5764,q5764,q6390,q6318,q6365,q4184,q4831,q6327,q6327,q6071,q6364,q5230,q5424,q5264,q5656,q5656,q5656,q5656,q5656,q5352,q5352,q4474,q6532,q6512,q1829,q2382,q2382,q2383,q2384,q2383,q2383,q9418,q4492,q6438,q6438,q5523,q8530,q3930,q5741,q5724,q8984,q5323,q4294,q4294,q5064,q5064,q5064,q9088,q6502,q5944,q5786,q8798,q8798,q8798,q5317,q5317,q9588,q3996,q9183,q4820,q4755,q5648,q9399,q9399,q7514,q7513,q184,q184,q153,q15,q11215)\n",
       "         +- Project [question_id#645, correct_answer#648]\n",
       "            +- ResolvedHint (strategy=broadcast)\n",
       "               +- Relation [question_id#645,bundle_id#646,explanation_id#647,correct_answer#648,part#649,tags#650,deployed_at#651L] csv\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n\u001B[0;32m<command-4170612332572212>\u001B[0m in \u001B[0;36m<cell line: 18>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     40\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     41\u001B[0m     \u001B[0;31m# Append to questions_df\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 42\u001B[0;31m     \u001B[0mquestions_df\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mquestions_df\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0munion\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrespond_responses2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     43\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     44\u001B[0m     \u001B[0;31m# Limit the number of files processed to 100\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m             \u001B[0mstart\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     47\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 48\u001B[0;31m                 \u001B[0mres\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     49\u001B[0m                 logger.log_success(\n\u001B[1;32m     50\u001B[0m                     \u001B[0mmodule_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mclass_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfunction_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mperf_counter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mstart\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msignature\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py\u001B[0m in \u001B[0;36munion\u001B[0;34m(self, other)\u001B[0m\n\u001B[1;32m   2546\u001B[0m         \u001B[0mAlso\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mstandard\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mSQL\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mthis\u001B[0m \u001B[0mfunction\u001B[0m \u001B[0mresolves\u001B[0m \u001B[0mcolumns\u001B[0m \u001B[0mby\u001B[0m \u001B[0mposition\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0;32mnot\u001B[0m \u001B[0mby\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2547\u001B[0m         \"\"\"\n\u001B[0;32m-> 2548\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mDataFrame\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0munion\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mother\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jdf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msparkSession\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2549\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2550\u001B[0m     \u001B[0;34m@\u001B[0m\u001B[0msince\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m1.3\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1319\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1320\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1321\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1322\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1323\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    200\u001B[0m                 \u001B[0;31m# Hide where the exception came from that shows a non-Pythonic\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    201\u001B[0m                 \u001B[0;31m# JVM exception message.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 202\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mconverted\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    203\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    204\u001B[0m                 \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mAnalysisException\u001B[0m: Union can only be performed on tables with the same number of columns, but the first table has 5 columns and the second table has 6 columns;\n'Union false, false\n:- LogicalRDD [question_id#618, correct_answer#619, user_answer#620, correct_flag#621, question_flag#622], false\n+- Project [user_answer#664, user_id#666, question_id#645, correct_answer#648, cast((user_answer#664 = correct_answer#648) as int) AS correct_flag#710, 1 AS question_flag#716]\n   +- Join Inner, (item_id#661 = question_id#645)\n      :- Filter (action_type#660 = respond)\n      :  +- Relation [timestamp#659,action_type#660,item_id#661,cursor_time#662L,source#663,user_answer#664,platform#665,user_id#666] csv\n      +- Filter question_id#645 IN (q4078,q6068,q6056,q4719,q6725,q6243,q5677,q4485,q4485,q5206,q6458,q4042,q342,q1190,q797,q997,q739,q1115,q1240,q1240,q925,q927,q200,q736,q1266,q6993,q6994,q6995,q6995,q6996,q6937,q6939,q6940,q6938,q6938,q6773,q6773,q6774,q6775,q6776,q6841,q6842,q6843,q6844,q604,q508,q776,q594,q856,q878,q1368,q1000,q447,q584,q584,q584,q1030,q462,q65,q65,q194,q154,q149,q8162,q115,q973,q1076,q406,q846,q1338,q312,q1045,q588,q1193,q956,q952,q651,q8140,q8057,q111,q8067,q160,q86,q1175,q1297,q854,q655,q655,q422,q696,q1190,q797,q997,q997,q739,q1240,q927,q736,q1266,q508,q763,q955,q216,q832,q835,q835,q835,q1101,q1219,q788,q1292,q576,q887,q539,q1213,q784,q1331,q774,q774,q583,q6650,q6651,q6652,q6649,q6650,q6836,q6833,q6833,q6834,q6834,q6834,q6835,q6913,q6913,q6914,q6915,q6916,q6809,q6812,q6812,q6812,q6810,q6811,q776,q594,q878,q584,q1030,q1030,q65,q149,q8162,q115,q406,q406,q1338,q312,q1045,q588,q1193,q1143,q1003,q1003,q949,q569,q1203,q1146,q1146,q1146,q732,q1224,q906,q1314,q1314,q1110,q1110,q1110,q355,q1554,q1556,q1555,q1555,q1555,q1556,q1556,q1758,q1759,q1760,q1759,q1759,q2439,q2440,q2441,q1632,q1633,q1634,q1634,q1722,q1723,q1724,q726,q439,q1218,q259,q803,q507,q1316,q590,q1376,q666,q566,q877,q1983,q1983,q1985,q1984,q2069,q2068,q2067,q1776,q1778,q1777,q1777,q1778,q2442,q2443,q2444,q2304,q2306,q2305,q956,q956,q952,q651,q8140,q8057,q8057,q160,q854,q655,q1250,q1114,q1336,q1339,q699,q724,q456,q1267,q1159,q1159,q752,q1187,q554,q2556,q2557,q2558,q2556,q1989,q1990,q1991,q2392,q2391,q2393,q1536,q1537,q1537,q1538,q1851,q1852,q1853,q1153,q905,q1251,q280,q1065,q217,q1280,q689,q1322,q292,q938,q1011,q1801,q1802,q1800,q1800,q1802,q2476,q2476,q2477,q2475,q2475,q1875,q1876,q1875,q1875,q1876,q1877,q1875,q2484,q2484,q2484,q2486,q2485,q2486,q2486,q2409,q2411,q2411,q2410,q2574,q2576,q2575,q2246,q2245,q2244,q1830,q1830,q1831,q1831,q1832,q1831,q1762,q1761,q1761,q1762,q1763,q1935,q1936,q1935,q1937,q1935,q1936,q722,q581,q285,q283,q1343,q1311,q1107,q397,q397,q1374,q1374,q251,q1234,q1234,q262,q696,q696,q1190,q1266,q740,q921,q633,q968,q375,q263,q263,q263,q763,q763,q216,q832,q835,q1292,q784,q2043,q2043,q2044,q2044,q2045,q1575,q1575,q1576,q1577,q2520,q2521,q2522,q2520,q2520,q1734,q1736,q1735,q1923,q1923,q1925,q1925,q1924,q561,q353,q353,q750,q573,q366,q423,q448,q291,q279,q1310,q612,q764,q764,q2505,q2505,q2505,q2506,q2505,q2507,q2379,q2380,q2381,q2380,q2381,q1791,q1792,q1793,q1793,q1785,q1786,q1786,q1786,q1787,q2340,q2341,q2342,q2340,q2340,q1677,q1678,q1678,q1679,q1766,q1765,q1764,q1764,q1764,q1890,q1891,q1891,q1892,q1616,q1616,q1614,q1615,q1548,q1549,q1550,q1550,q767,q773,q985,q289,q336,q634,q373,q799,q261,q261,q502,q1155,q915,q1917,q1919,q1918,q1918,q1918,q1919,q1918,q2166,q2166,q2167,q2168,q2168,q2166,q2167,q2168,q1608,q1610,q1609,q1609,q1610,q1610,q1587,q1587,q1589,q1589,q1589,q1588,q2259,q2260,q2261,q2261,q948,q392,q982,q442,q1345,q380,q895,q589,q848,q1111,q585,q639,q2397,q2397,q2397,q2398,q2398,q2398,q2398,q2399,q2399,q2362,q2363,q2361,q2363,q2363,q2363,q2363,q2362,q1953,q1954,q1955,q1955,q1955,q1955,q1545,q1547,q1546,q1546,q1546,q2226,q2227,q2227,q2227,q2228,q2226,q7861,q7865,q7864,q7863,q7862,q7475,q7476,q7477,q7478,q7479,q7478,q7054,q7056,q7057,q7057,q7058,q7058,q7055,q56,q8172,q20,q11075,q13,q13,q13,q120,q8152,q11106,q16,q8179,q8096,q8096,q8096,q11095,q193,q8134,q8123,q11143,q108,q165,q165,q8101,q11238,q608,q608,q719,q309,q11262,q11262,q11262,q494,q1015,q790,q11259,q350,q586,q586,q2343,q2344,q2345,q1926,q1927,q1928,q2547,q2548,q2549,q1740,q1741,q1742,q2295,q2296,q2297,q2823,q2823,q2825,q2824,q2825,q3115,q3116,q3114,q3414,q3415,q3416,q3416,q3416,q3450,q3452,q3451,q3450,q3450,q3451,q3451,q3452,q5054,q5054,q5780,q6360,q3114,q3115,q3116,q3116,q2823,q2824,q2825,q2928,q7402,q7402,q7402,q7402,q2208,q2208,q2208,q8095,q9007,q5530,q5530,q5530,q4827,q4526,q9114,q9114,q9114,q9114,q9114,q4468,q4128,q4128,q4128,q4631,q4631,q17386,q17386,q4008,q938,q2226,q3629,q6331,q6523,q6523,q6523,q6523,q6523,q6523,q3774,q3774,q3774,q5031,q3928,q4497,q5044,q5784,q5784,q5784,q5784,q6180,q6180,q6180,q5063,q4672,q5662,q3831,q5094,q5094,q5094,q5094,q5094,q5220,q5220,q4865,q17063,q17063,q5769,q5769,q5314,q4313,q4313,q4313,q8789,q6469,q6469,q5934,q5934,q6360,q5475,q1971,q1972,q1973,q1972,q2001,q2001,q2003,q2002,q1618,q1617,q1617,q1618,q1618,q1619,q2454,q2454,q2455,q2455,q2456,q2456,q1986,q1987,q1988,q1987,q1986,q6031,q6031,q6031,q6038,q6038,q6038,q4931,q4931,q4931,q4792,q4886,q4886,q6544,q6544,q3911,q3911,q5337,q5337,q5337,q5337,q6185,q5040,q5040,q6225,q10020,q5046,q5740,q5740,q5682,q5682,q5682,q5682,q4181,q4314,q4314,q6453,q6453,q6453,q5630,q5630,q4602,q5267,q9578,q9578,q9578,q9578,q9578,q9578,q5731,q5632,q4022,q6746,q5060,q6114,q6183,q6183,q5065,q1596,q1597,q1598,q1598,q2508,q2509,q2510,q1962,q1963,q1963,q1963,q1964,q2188,q2189,q2189,q2187,q2091,q2092,q2093,q2093,q5052,q4045,q4045,q3937,q3937,q5737,q5737,q5787,q5686,q5057,q5057,q4527,q4478,q4325,q4325,q6171,q6171,q6171,q6171,q5315,q5315,q5315,q3698,q5764,q5764,q5764,q5764,q5764,q6390,q6318,q6365,q4184,q4831,q6327,q6327,q6071,q6364,q5230,q5424,q5264,q5656,q5656,q5656,q5656,q5656,q5352,q5352,q4474,q6532,q6512,q1829,q2382,q2382,q2383,q2384,q2383,q2383,q9418,q4492,q6438,q6438,q5523,q8530,q3930,q5741,q5724,q8984,q5323,q4294,q4294,q5064,q5064,q5064,q9088,q6502,q5944,q5786,q8798,q8798,q8798,q5317,q5317,q9588,q3996,q9183,q4820,q4755,q5648,q9399,q9399,q7514,q7513,q184,q184,q153,q15,q11215)\n         +- Project [question_id#645, correct_answer#648]\n            +- ResolvedHint (strategy=broadcast)\n               +- Relation [question_id#645,bundle_id#646,explanation_id#647,correct_answer#648,part#649,tags#650,deployed_at#651L] csv\n",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: Union can only be performed on tables with the same number of columns, but the first table has 5 columns and the second table has 6 columns;\n'Union false, false\n:- LogicalRDD [question_id#618, correct_answer#619, user_answer#620, correct_flag#621, question_flag#622], false\n+- Project [user_answer#664, user_id#666, question_id#645, correct_answer#648, cast((user_answer#664 = correct_answer#648) as int) AS correct_flag#710, 1 AS question_flag#716]\n   +- Join Inner, (item_id#661 = question_id#645)\n      :- Filter (action_type#660 = respond)\n      :  +- Relation [timestamp#659,action_type#660,item_id#661,cursor_time#662L,source#663,user_answer#664,platform#665,user_id#666] csv\n      +- Filter question_id#645 IN (q4078,q6068,q6056,q4719,q6725,q6243,q5677,q4485,q4485,q5206,q6458,q4042,q342,q1190,q797,q997,q739,q1115,q1240,q1240,q925,q927,q200,q736,q1266,q6993,q6994,q6995,q6995,q6996,q6937,q6939,q6940,q6938,q6938,q6773,q6773,q6774,q6775,q6776,q6841,q6842,q6843,q6844,q604,q508,q776,q594,q856,q878,q1368,q1000,q447,q584,q584,q584,q1030,q462,q65,q65,q194,q154,q149,q8162,q115,q973,q1076,q406,q846,q1338,q312,q1045,q588,q1193,q956,q952,q651,q8140,q8057,q111,q8067,q160,q86,q1175,q1297,q854,q655,q655,q422,q696,q1190,q797,q997,q997,q739,q1240,q927,q736,q1266,q508,q763,q955,q216,q832,q835,q835,q835,q1101,q1219,q788,q1292,q576,q887,q539,q1213,q784,q1331,q774,q774,q583,q6650,q6651,q6652,q6649,q6650,q6836,q6833,q6833,q6834,q6834,q6834,q6835,q6913,q6913,q6914,q6915,q6916,q6809,q6812,q6812,q6812,q6810,q6811,q776,q594,q878,q584,q1030,q1030,q65,q149,q8162,q115,q406,q406,q1338,q312,q1045,q588,q1193,q1143,q1003,q1003,q949,q569,q1203,q1146,q1146,q1146,q732,q1224,q906,q1314,q1314,q1110,q1110,q1110,q355,q1554,q1556,q1555,q1555,q1555,q1556,q1556,q1758,q1759,q1760,q1759,q1759,q2439,q2440,q2441,q1632,q1633,q1634,q1634,q1722,q1723,q1724,q726,q439,q1218,q259,q803,q507,q1316,q590,q1376,q666,q566,q877,q1983,q1983,q1985,q1984,q2069,q2068,q2067,q1776,q1778,q1777,q1777,q1778,q2442,q2443,q2444,q2304,q2306,q2305,q956,q956,q952,q651,q8140,q8057,q8057,q160,q854,q655,q1250,q1114,q1336,q1339,q699,q724,q456,q1267,q1159,q1159,q752,q1187,q554,q2556,q2557,q2558,q2556,q1989,q1990,q1991,q2392,q2391,q2393,q1536,q1537,q1537,q1538,q1851,q1852,q1853,q1153,q905,q1251,q280,q1065,q217,q1280,q689,q1322,q292,q938,q1011,q1801,q1802,q1800,q1800,q1802,q2476,q2476,q2477,q2475,q2475,q1875,q1876,q1875,q1875,q1876,q1877,q1875,q2484,q2484,q2484,q2486,q2485,q2486,q2486,q2409,q2411,q2411,q2410,q2574,q2576,q2575,q2246,q2245,q2244,q1830,q1830,q1831,q1831,q1832,q1831,q1762,q1761,q1761,q1762,q1763,q1935,q1936,q1935,q1937,q1935,q1936,q722,q581,q285,q283,q1343,q1311,q1107,q397,q397,q1374,q1374,q251,q1234,q1234,q262,q696,q696,q1190,q1266,q740,q921,q633,q968,q375,q263,q263,q263,q763,q763,q216,q832,q835,q1292,q784,q2043,q2043,q2044,q2044,q2045,q1575,q1575,q1576,q1577,q2520,q2521,q2522,q2520,q2520,q1734,q1736,q1735,q1923,q1923,q1925,q1925,q1924,q561,q353,q353,q750,q573,q366,q423,q448,q291,q279,q1310,q612,q764,q764,q2505,q2505,q2505,q2506,q2505,q2507,q2379,q2380,q2381,q2380,q2381,q1791,q1792,q1793,q1793,q1785,q1786,q1786,q1786,q1787,q2340,q2341,q2342,q2340,q2340,q1677,q1678,q1678,q1679,q1766,q1765,q1764,q1764,q1764,q1890,q1891,q1891,q1892,q1616,q1616,q1614,q1615,q1548,q1549,q1550,q1550,q767,q773,q985,q289,q336,q634,q373,q799,q261,q261,q502,q1155,q915,q1917,q1919,q1918,q1918,q1918,q1919,q1918,q2166,q2166,q2167,q2168,q2168,q2166,q2167,q2168,q1608,q1610,q1609,q1609,q1610,q1610,q1587,q1587,q1589,q1589,q1589,q1588,q2259,q2260,q2261,q2261,q948,q392,q982,q442,q1345,q380,q895,q589,q848,q1111,q585,q639,q2397,q2397,q2397,q2398,q2398,q2398,q2398,q2399,q2399,q2362,q2363,q2361,q2363,q2363,q2363,q2363,q2362,q1953,q1954,q1955,q1955,q1955,q1955,q1545,q1547,q1546,q1546,q1546,q2226,q2227,q2227,q2227,q2228,q2226,q7861,q7865,q7864,q7863,q7862,q7475,q7476,q7477,q7478,q7479,q7478,q7054,q7056,q7057,q7057,q7058,q7058,q7055,q56,q8172,q20,q11075,q13,q13,q13,q120,q8152,q11106,q16,q8179,q8096,q8096,q8096,q11095,q193,q8134,q8123,q11143,q108,q165,q165,q8101,q11238,q608,q608,q719,q309,q11262,q11262,q11262,q494,q1015,q790,q11259,q350,q586,q586,q2343,q2344,q2345,q1926,q1927,q1928,q2547,q2548,q2549,q1740,q1741,q1742,q2295,q2296,q2297,q2823,q2823,q2825,q2824,q2825,q3115,q3116,q3114,q3414,q3415,q3416,q3416,q3416,q3450,q3452,q3451,q3450,q3450,q3451,q3451,q3452,q5054,q5054,q5780,q6360,q3114,q3115,q3116,q3116,q2823,q2824,q2825,q2928,q7402,q7402,q7402,q7402,q2208,q2208,q2208,q8095,q9007,q5530,q5530,q5530,q4827,q4526,q9114,q9114,q9114,q9114,q9114,q4468,q4128,q4128,q4128,q4631,q4631,q17386,q17386,q4008,q938,q2226,q3629,q6331,q6523,q6523,q6523,q6523,q6523,q6523,q3774,q3774,q3774,q5031,q3928,q4497,q5044,q5784,q5784,q5784,q5784,q6180,q6180,q6180,q5063,q4672,q5662,q3831,q5094,q5094,q5094,q5094,q5094,q5220,q5220,q4865,q17063,q17063,q5769,q5769,q5314,q4313,q4313,q4313,q8789,q6469,q6469,q5934,q5934,q6360,q5475,q1971,q1972,q1973,q1972,q2001,q2001,q2003,q2002,q1618,q1617,q1617,q1618,q1618,q1619,q2454,q2454,q2455,q2455,q2456,q2456,q1986,q1987,q1988,q1987,q1986,q6031,q6031,q6031,q6038,q6038,q6038,q4931,q4931,q4931,q4792,q4886,q4886,q6544,q6544,q3911,q3911,q5337,q5337,q5337,q5337,q6185,q5040,q5040,q6225,q10020,q5046,q5740,q5740,q5682,q5682,q5682,q5682,q4181,q4314,q4314,q6453,q6453,q6453,q5630,q5630,q4602,q5267,q9578,q9578,q9578,q9578,q9578,q9578,q5731,q5632,q4022,q6746,q5060,q6114,q6183,q6183,q5065,q1596,q1597,q1598,q1598,q2508,q2509,q2510,q1962,q1963,q1963,q1963,q1964,q2188,q2189,q2189,q2187,q2091,q2092,q2093,q2093,q5052,q4045,q4045,q3937,q3937,q5737,q5737,q5787,q5686,q5057,q5057,q4527,q4478,q4325,q4325,q6171,q6171,q6171,q6171,q5315,q5315,q5315,q3698,q5764,q5764,q5764,q5764,q5764,q6390,q6318,q6365,q4184,q4831,q6327,q6327,q6071,q6364,q5230,q5424,q5264,q5656,q5656,q5656,q5656,q5656,q5352,q5352,q4474,q6532,q6512,q1829,q2382,q2382,q2383,q2384,q2383,q2383,q9418,q4492,q6438,q6438,q5523,q8530,q3930,q5741,q5724,q8984,q5323,q4294,q4294,q5064,q5064,q5064,q9088,q6502,q5944,q5786,q8798,q8798,q8798,q5317,q5317,q9588,q3996,q9183,q4820,q4755,q5648,q9399,q9399,q7514,q7513,q184,q184,q153,q15,q11215)\n         +- Project [question_id#645, correct_answer#648]\n            +- ResolvedHint (strategy=broadcast)\n               +- Relation [question_id#645,bundle_id#646,explanation_id#647,correct_answer#648,part#649,tags#650,deployed_at#651L] csv\n",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # OPTIMIZED CODE\n",
    "# from pyspark.sql.functions import col, lit, broadcast\n",
    "# from pyspark.sql.types import IntegerType\n",
    "# from pyspark.sql import SparkSession\n",
    "\n",
    "# spark = SparkSession.builder.appName(\"Capstone606\").getOrCreate()\n",
    "\n",
    "# # Create an empty Spark DataFrame with the same schema as main_df\n",
    "# schema = 'question_id STRING, correct_answer STRING, user_answer STRING, correct_flag INT, question_flag INT'\n",
    "# file_schema = \"timestamp STRING, action_type STRING, item_id STRING, cursor_time LONG, source STRING, user_answer STRING, platform STRING, user_id STRING\"\n",
    "# questions_df = spark.createDataFrame([], schema)\n",
    "\n",
    "# # Read questions.csv as a Spark DataFrame and broadcast it\n",
    "# questionsdf = spark.read.csv(\"/mnt/capstone606/questionsandpayments/questions.csv\",header=True,inferSchema=True)\n",
    "# questionsdf_broadcast = broadcast(questionsdf)\n",
    "\n",
    "# # Loop through files in the EdNet-KT4/KT4 directory\n",
    "# for i in dbutils.fs.ls(\"/mnt/capstone606/clickstreamfiles/\"):\n",
    "#     # Read the file as a Spark DataFrame\n",
    "#     df = spark.read.csv(path=i.path, header=True, schema=file_schema)\n",
    "\n",
    "#     # Filter by action_type == 'respond'\n",
    "#     respond_responses = df.filter(\"action_type = 'respond'\")\n",
    "\n",
    "#     # Join with questionsdf on question_id to get correct_answer\n",
    "#     rr = respond_responses.select('item_id').rdd.flatMap(lambda x: x).collect()\n",
    "#     tempdf = questionsdf_broadcast.select('question_id', 'correct_answer').\\\n",
    "#              filter(col('question_id').isin(rr))\n",
    "    \n",
    "#     respond_responses2 = respond_responses.join(tempdf, respond_responses.item_id == tempdf.question_id)\n",
    "\n",
    "#     # Drop unnecessary columns and rename user_answer to match main_df\n",
    "#     respond_responses2 = respond_responses2.drop('item_id', 'timestamp', 'cursor_time', 'source', 'action_type', 'platform').\\\n",
    "#                          withColumnRenamed('user_response', 'user_answer')\n",
    "\n",
    "#     # Compute correct_flag and question_flag\n",
    "#     respond_responses2 = respond_responses2.withColumn('correct_flag', \n",
    "#                              (col('user_answer') == col('correct_answer')).cast(IntegerType())).\\\n",
    "#                          withColumn('question_flag', lit(1))\n",
    "\n",
    "#     # Append to questions_df\n",
    "#     questions_df = questions_df.union(respond_responses2)\n",
    "\n",
    "#     # Limit the number of files processed to 100\n",
    "#     if questions_df.count() >= 100:\n",
    "#         break\n",
    "\n",
    "# # Cache the questions_df DataFrame\n",
    "# questions_df.cache()\n",
    "# questions_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18e99796-be3c-4135-86e7-c8676b1295e2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+-----------+------------+-------------+\n|question_id|correct_answer|user_answer|correct_flag|question_flag|\n+-----------+--------------+-----------+------------+-------------+\n|        q13|             a|          a|           1|            1|\n|        q13|             a|          b|           0|            1|\n|        q13|             a|          a|           1|            1|\n|        q15|             d|          d|           1|            1|\n|        q16|             c|          c|           1|            1|\n|        q20|             b|          a|           0|            1|\n|        q56|             c|          d|           0|            1|\n|        q65|             b|          b|           1|            1|\n|        q65|             b|          d|           0|            1|\n|        q65|             b|          c|           0|            1|\n|        q86|             a|          a|           1|            1|\n|       q108|             a|          a|           1|            1|\n|       q111|             a|          a|           1|            1|\n|       q115|             a|          a|           1|            1|\n|       q115|             a|          c|           0|            1|\n|       q120|             b|          b|           1|            1|\n|       q149|             b|          b|           1|            1|\n|       q149|             b|          d|           0|            1|\n|       q153|             a|          a|           1|            1|\n|       q154|             b|          b|           1|            1|\n+-----------+--------------+-----------+------------+-------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "questions_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cf4c817-a5f9-46f6-8990-2a55091a38bf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/capstone606 has been unmounted.\nOut[8]: True"
     ]
    }
   ],
   "source": [
    "# UNMOUNT\n",
    "dbutils.fs.unmount(\"/mnt/capstone606\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a66efd92-fff8-48e0-b22b-583571b5b49c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Notebook1",
   "notebookOrigID": 3420688240022826,
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
